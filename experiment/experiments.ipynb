{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b09b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"http://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00aeede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64eee",
   "metadata": {},
   "source": [
    "### basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0384593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing functions\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def removing_numbers(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def lower_case(text):\n",
    "    return \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('Ø›', \"\")\n",
    "    return re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "def removing_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_small_sentences(df):\n",
    "    df['content'] = df['content'].apply(lambda x: np.nan if len(str(x).split()) < 3 else x)\n",
    "    return df\n",
    "\n",
    "def normalize_text(df):\n",
    "    try:\n",
    "        df['content'] = df['content'].apply(lower_case)\n",
    "        df['content'] = df['content'].apply(remove_stop_words)\n",
    "        df['content'] = df['content'].apply(removing_numbers)\n",
    "        df['content'] = df['content'].apply(removing_punctuations)\n",
    "        df['content'] = df['content'].apply(removing_urls)\n",
    "        df['content'] = df['content'].apply(lemmatization)\n",
    "        df = remove_small_sentences(df)\n",
    "        return df.dropna(subset=['content'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba38c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca991b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sentiment'].isin(['happiness','sadness'])\n",
    "df = df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaee5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb56acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>1957392626</td>\n",
       "      <td>0</td>\n",
       "      <td>itstayloryall im sad missed family reunion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37228</th>\n",
       "      <td>1753450968</td>\n",
       "      <td>0</td>\n",
       "      <td>asked sing yet again aha coffee mornin time ps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>1960762036</td>\n",
       "      <td>0</td>\n",
       "      <td>lolzyluvsjb neither could i looked ebay like e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32016</th>\n",
       "      <td>1752157659</td>\n",
       "      <td>1</td>\n",
       "      <td>eplisa diaper feeding cooking stuff hahaha yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30594</th>\n",
       "      <td>1751644693</td>\n",
       "      <td>1</td>\n",
       "      <td>justinsxe woot woot super cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  sentiment  \\\n",
       "1767   1957392626          0   \n",
       "37228  1753450968          0   \n",
       "5360   1960762036          0   \n",
       "32016  1752157659          1   \n",
       "30594  1751644693          1   \n",
       "\n",
       "                                                 content  \n",
       "1767          itstayloryall im sad missed family reunion  \n",
       "37228  asked sing yet again aha coffee mornin time ps...  \n",
       "5360   lolzyluvsjb neither could i looked ebay like e...  \n",
       "32016  eplisa diaper feeding cooking stuff hahaha yea...  \n",
       "30594                     justinsxe woot woot super cool  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2abf9",
   "metadata": {},
   "source": [
    "### apply BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 10)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96356543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388fb43",
   "metadata": {},
   "source": [
    "#### logging experiment on mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2ca222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as iamprashantjain\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as iamprashantjain\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"iamprashantjain/Emotion-Detection-MLOps\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"iamprashantjain/Emotion-Detection-MLOps\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository iamprashantjain/Emotion-Detection-MLOps initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository iamprashantjain/Emotion-Detection-MLOps initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 22:44:54 INFO mlflow.tracking.fluent: Experiment with name 'baseline model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/c746a5e75424444387ff33ca4afbd667', creation_time=1753031695025, experiment_id='1', last_update_time=1753031695025, lifecycle_stage='active', name='baseline model', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "dagshub.init(repo_owner='iamprashantjain', repo_name='Emotion-Detection-MLOps', mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/iamprashantjain/Emotion-Detection-MLOps.mlflow\")\n",
    "mlflow.set_experiment(\"baseline model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666fa07",
   "metadata": {},
   "source": [
    "#### run mlflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a667664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\Emotion-Detection-MLOps\\venv\\lib\\site-packages\\_distutils_hack\\__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5667\n",
      "Precision: 0.6487\n",
      "Recall: 0.2812\n",
      "F1 Score: 0.3924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log preprocessing params\n",
    "    mlflow.log_param(\"vectorizer\", \"BOW\")\n",
    "    mlflow.log_param(\"num_features\", 10)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "    # Model building & training\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log model params\n",
    "    mlflow.log_param(\"model\", \"logistic regression\")\n",
    "\n",
    "    # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Optionally log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Log the notebook\n",
    "    import os\n",
    "    notebook_path = \"experiments.ipynb\"\n",
    "    os.system(f\"jupyter nbconvert --to notebook --execute --inplace{notebook_path}\")\n",
    "    mlflow.log_artifact(notebook_path)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d2e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
