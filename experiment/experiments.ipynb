{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a182bc",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b09b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"http://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00aeede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64eee",
   "metadata": {},
   "source": [
    "### basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0384593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iampr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text preprocessing functions\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def removing_numbers(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def lower_case(text):\n",
    "    return \" \".join([word.lower() for word in text.split()])\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('Ø›', \"\")\n",
    "    return re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "def removing_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_small_sentences(df):\n",
    "    df['content'] = df['content'].apply(lambda x: np.nan if len(str(x).split()) < 3 else x)\n",
    "    return df\n",
    "\n",
    "def normalize_text(df):\n",
    "    try:\n",
    "        df['content'] = df['content'].apply(lower_case)\n",
    "        df['content'] = df['content'].apply(remove_stop_words)\n",
    "        df['content'] = df['content'].apply(removing_numbers)\n",
    "        df['content'] = df['content'].apply(removing_punctuations)\n",
    "        df['content'] = df['content'].apply(removing_urls)\n",
    "        df['content'] = df['content'].apply(lemmatization)\n",
    "        df = remove_small_sentences(df)\n",
    "        return df.dropna(subset=['content'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba38c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca991b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sentiment'].isin(['happiness','sadness'])\n",
    "df = df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaee5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fb56acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7899</th>\n",
       "      <td>1961922392</td>\n",
       "      <td>0</td>\n",
       "      <td>skype call billie webcam dont work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22376</th>\n",
       "      <td>1694419708</td>\n",
       "      <td>1</td>\n",
       "      <td>drchino yay joy department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24125</th>\n",
       "      <td>1694769245</td>\n",
       "      <td>1</td>\n",
       "      <td>happy star war day everyone excuse drinkies eh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>1957549892</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed favourite heel fell wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26567</th>\n",
       "      <td>1695443815</td>\n",
       "      <td>1</td>\n",
       "      <td>diannemr like that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  sentiment                                         content\n",
       "7899   1961922392          0              skype call billie webcam dont work\n",
       "22376  1694419708          1                      drchino yay joy department\n",
       "24125  1694769245          1  happy star war day everyone excuse drinkies eh\n",
       "2462   1957549892          0                  fixed favourite heel fell wear\n",
       "26567  1695443815          1                              diannemr like that"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2abf9",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2564e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 10)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96356543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388fb43",
   "metadata": {},
   "source": [
    "#### logging experiment on mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2ca222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"iamprashantjain/Emotion-Detection-MLOps\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"iamprashantjain/Emotion-Detection-MLOps\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository iamprashantjain/Emotion-Detection-MLOps initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository iamprashantjain/Emotion-Detection-MLOps initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 23:20:04 INFO mlflow.tracking.fluent: Experiment with name 'baseline_model' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/a5492073f5de4e03875e2572ddb4e014', creation_time=1753033805239, experiment_id='3', last_update_time=1753033805239, lifecycle_stage='active', name='baseline_model', tags={}>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "dagshub.init(repo_owner='iamprashantjain', repo_name='Emotion-Detection-MLOps', mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/iamprashantjain/Emotion-Detection-MLOps.mlflow\")\n",
    "mlflow.set_experiment(\"baseline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666fa07",
   "metadata": {},
   "source": [
    "#### run mlflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a667664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\Emotion-Detection-MLOps\\venv\\lib\\site-packages\\_distutils_hack\\__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5667\n",
      "Precision: 0.6487\n",
      "Recall: 0.2812\n",
      "F1 Score: 0.3924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log preprocessing params\n",
    "    mlflow.log_param(\"vectorizer\", \"BOW\")\n",
    "    mlflow.log_param(\"num_features\", 10)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "    # Model building & training\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Log model params\n",
    "    mlflow.log_param(\"model\", \"logistic regression\")\n",
    "\n",
    "    # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Optionally log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    # Log the notebook\n",
    "    import os\n",
    "    notebook_path = \"experiments.ipynb\"\n",
    "    os.system(f\"jupyter nbconvert --to notebook --execute --inplace{notebook_path}\")\n",
    "    mlflow.log_artifact(notebook_path)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82011b9",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "apply mulitple algorithm with both vectorizer (bow & tfidf) to find which combination is giving best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "355d2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 23:20:56 INFO mlflow.tracking.fluent: Experiment with name 'BOW V/S TFIDF' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/7ece5e3805584630972fe0dd04783876', creation_time=1753033856903, experiment_id='4', last_update_time=1753033856903, lifecycle_stage='active', name='BOW V/S TFIDF', tags={}>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "mlflow.set_experiment(\"BOW V/S TFIDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f04c0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=200),\n",
    "    \"SVC\": SVC(probability=True),\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "vectorizers = {\n",
    "    \"BOW\": CountVectorizer(max_features=10),\n",
    "    \"TFIDF\": TfidfVectorizer(max_features=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2ed35f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression + BOW] F1 Score: 0.3924\n",
      "[LogisticRegression + TFIDF] F1 Score: 0.3955\n",
      "[SVC + BOW] F1 Score: 0.3827\n",
      "[SVC + TFIDF] F1 Score: 0.3787\n",
      "[NaiveBayes + BOW] F1 Score: 0.6406\n",
      "[NaiveBayes + TFIDF] F1 Score: 0.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\Emotion-Detection-MLOps\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:35:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost + BOW] F1 Score: 0.3764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\CampusX_DS\\campusx_dsmp2\\9. MLOps revisited\\tutorial\\Emotion-Detection-MLOps\\venv\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [23:36:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost + TFIDF] F1 Score: 0.3744\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = \"All Combination Experiment\") as parent_run:\n",
    "    for model_name, model in models.items():\n",
    "        for vectorizer_name, vectorizer in vectorizers.items():\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_{vectorizer_name}\", nested=True):\n",
    "\n",
    "                X = vectorizer.fit_transform(df['content'])\n",
    "                y = df['sentiment']\n",
    "\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "                # Preprocessing parameters\n",
    "                mlflow.log_param(\"model_name\", model_name)\n",
    "                mlflow.log_param(\"vectorizer\", vectorizer_name)\n",
    "                mlflow.log_param(\"num_features\", 10)\n",
    "                mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "                # Model-specific parameter logging\n",
    "                if model_name == \"LogisticRegression\":\n",
    "                    mlflow.log_param(\"solver\", model.solver if hasattr(model, \"solver\") else \"lbfgs\")\n",
    "                    mlflow.log_param(\"max_iter\", model.max_iter)\n",
    "                elif model_name == \"SVC\":\n",
    "                    mlflow.log_param(\"kernel\", model.kernel if hasattr(model, \"kernel\") else \"rbf\")\n",
    "                    mlflow.log_param(\"C\", model.C)\n",
    "                elif model_name == \"NaiveBayes\":\n",
    "                    mlflow.log_param(\"alpha\", model.alpha)\n",
    "                elif model_name == \"XGBoost\":\n",
    "                    mlflow.log_param(\"n_estimators\", model.n_estimators)\n",
    "                    mlflow.log_param(\"learning_rate\", model.learning_rate)\n",
    "                    mlflow.log_param(\"max_depth\", model.max_depth)\n",
    "\n",
    "                # Fit model\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Predictions\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # Evaluation metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"precision\", precision)\n",
    "                mlflow.log_metric(\"recall\", recall)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "                # Log the model\n",
    "                mlflow.sklearn.log_model(model, artifact_path=\"model\")\n",
    "                \n",
    "                # Log the notebook\n",
    "                import os\n",
    "                notebook_path = \"experiments.ipynb\"\n",
    "                os.system(f\"jupyter nbconvert --to notebook --execute --inplace{notebook_path}\")\n",
    "                mlflow.log_artifact(notebook_path)\n",
    "                \n",
    "                print(f\"[{model_name} + {vectorizer_name}] F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7fd3d8",
   "metadata": {},
   "source": [
    "### Experiment 3 - Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac592aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will take - bow + lr\n",
    "#find best params of best model & vectorizer combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4e8be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Test F1 Score with Best Model: 0.6434\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear'],  # 'liblinear' supports l1 and l2\n",
    "    'max_iter': [100, 200]\n",
    "}\n",
    "\n",
    "# MLflow Tracking\n",
    "mlflow.set_experiment(\"BOW_LR_Hyperparameter_tuning\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Parent_Run\") as parent_run:\n",
    "    \n",
    "    # Hyperparameter Tuning\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Log each param combination as a child run\n",
    "    for idx, params in enumerate(grid_search.cv_results_['params']):\n",
    "        mean_score = grid_search.cv_results_['mean_test_score'][idx]\n",
    "        std_score = grid_search.cv_results_['std_test_score'][idx]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"LR_{params}\", nested=True):\n",
    "            model = LogisticRegression(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_metric(\"mean_cv_f1\", mean_score)\n",
    "            mlflow.log_metric(\"std_cv_f1\", std_score)\n",
    "            mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"test_precision\", precision)\n",
    "            mlflow.log_metric(\"test_recall\", recall)\n",
    "            mlflow.log_metric(\"test_f1\", f1)\n",
    "\n",
    "    # Log best model details in parent run\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"best_cv_f1\", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate best model on test set\n",
    "    best_pred = best_model.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, best_pred, zero_division=0)\n",
    "    mlflow.log_metric(\"best_test_f1\", test_f1)\n",
    "\n",
    "    mlflow.sklearn.log_model(best_model, artifact_path=\"best_model\")\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Test F1 Score with Best Model: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcf73b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
