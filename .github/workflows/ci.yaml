name: CI Pipeline

on:
  push:   # Trigger pipeline on every push event to the repository

jobs:
  project-testing:
    runs-on: ubuntu-latest   # Use latest GitHub-hosted Ubuntu runner

    env:  # GLOBAL ENVIRONMENT VARIABLES for all steps
      PYTHONPATH: ${{ github.workspace }}   # Allows Python to resolve modules inside 'src' folder
      AWS_REGION: ap-south-1

    steps:
      # STEP 1: Checkout code from GitHub
      - name: Checkout code
        uses: actions/checkout@v4

      # STEP 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # STEP 3: Cache pip dependencies for faster pipeline runs
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements_dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # STEP 4: Install required Python packages
      - name: Install Python dependencies (quiet & faster)
        run: |
          python -m pip install --upgrade pip --quiet
          pip install -r requirements_dev.txt --quiet

      # STEP 5: Install DVC and Dagshub for data versioning and remote storage
      - name: Setup DVC and Dagshub
        run: |
          pip install --quiet dvc[s3] dagshub

      # STEP 6: Pull dataset and artifacts tracked by DVC from S3
      - name: Pull DVC data from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc pull --quiet
          test -f artifacts/data/vectorized/test_vectorized.csv   # Validate if key file is pulled

      # STEP 7: Re-run the entire data pipeline using DVC
      - name: Run DVC pipeline
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc repro --quiet


      - name: Run model loading tests
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          python -m unittest discover -s tests -p "test_model.py"

      # STEP 9: Promote latest best model to Production stage in MLflow Registry
      - name: Promote model to production
        if: success()
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          python src/utils/utils.py

      # STEP 10: Unit test Flask application for health and predict endpoints
      - name: Run Flask app tests
        if: success()
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          python -m unittest tests/test_flask_app.py

      # STEP 11: Login to AWS ECR using credentials securely stored in GitHub Secrets
      - name: ECR Login
        if: success()
        run: |
          aws configure set region $AWS_REGION
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com

      # STEP 12: Build Docker image
      - name: Build docker image
        if: success()
        run: |
          docker build -t prashant-mlops-ecr .

      # STEP 13: Tag Docker image before pushing to AWS ECR
      - name: Tag docker image
        if: success()
        run: |
          docker tag prashant-mlops-ecr:latest 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com/prashant-mlops-ecr:latest

      # STEP 14: Push Docker image to AWS Elastic Container Registry (ECR)
      - name: Docker push to ECR
        if: success()
        run: |
          docker push 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com/prashant-mlops-ecr:latest
      

      # STEP 15: Deploy on EC2 (pull image in EC2 and run)
      - name: Deploy to EC2
        if: success()
        uses: appleboy/ssh-action@v0.1.5 
        with:
          host: ${{ secrets.EC2_HOST }}
          username: ${{ secrets.EC2_USER }}
          key: ${{ secrets.EC2_SSH_KEY }}
          envs: AWS_REGION
          script: |
            set -e
            if ! command -v docker &> /dev/null
            then
              sudo apt update
              sudo apt install docker.io -y
              sudo systemctl enable docker
              sudo systemctl start docker
            fi
            
            aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
            aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            aws configure set default.region $AWS_REGION
            
            aws ecr get-login-password --region $AWS_REGION | sudo docker login --username AWS --password-stdin 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com
            
            sudo docker pull 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com/prashant-mlops-ecr:latest
            
            sudo docker stop my-app || true
            sudo docker rm my-app || true
            
            sudo docker run -d -p 5000:5000 --name my-app -e DAGSHUB_PAT=${{ secrets.DAGSHUB_PAT }} 739275446561.dkr.ecr.$AWS_REGION.amazonaws.com/prashant-mlops-ecr:latest