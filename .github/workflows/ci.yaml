name: CI Pipeline

on: 
  push:   # Trigger pipeline on every push to the repository

jobs:
  project-testing:
    runs-on: ubuntu-latest   # Use the latest Ubuntu GitHub-hosted runner

    env:  # GLOBAL ENVIRONMENT VARIABLES for all steps
      PYTHONPATH: ${{ github.workspace }}   # Allows Python to resolve 'src' modules across all steps

    steps:
      - name: Checkout code
        uses: actions/checkout@v4   # Checkout your code from the GitHub repo

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'   # Set Python version

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements_dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
        # Cache pip packages for faster builds

      - name: Install Python dependencies (quiet & faster)
        run: |
          python -m pip install --upgrade pip --quiet
          pip install -r requirements_dev.txt --quiet
        # Install your Python dependencies quietly to reduce logs and improve speed

      - name: Setup DVC and Dagshub
        run: |
          pip install --quiet dvc[s3] dagshub
        # Install DVC with S3 support and Dagshub client

      - name: Pull DVC data from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc pull --quiet   # Pull all tracked data/artifacts from DVC S3 remote storage
          test -f artifacts/data/vectorized/test_vectorized.csv   # Check if key artifact exists after pull

      - name: Run DVC pipeline
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc repro --quiet   # Run full DVC pipeline to reproduce all stages

      - name: Run model loading tests
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          python -m unittest discover -s tests -p "test_model.py"
        # Run all unittests matching test_model.py inside tests folder

      - name: Promote model to production
        if: success()   # Promote only if all previous steps succeed
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: python src/utils/utils.py   # Promote the latest model to Production stage on MLflow
