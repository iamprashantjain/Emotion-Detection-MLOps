name: CI Pipeline

on: 
  push:

jobs:
  project-testing:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements_dev.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies (quiet & faster)
        run: |
          python -m pip install --upgrade pip --quiet
          pip install -r requirements_dev.txt --quiet

      - name: Setup DVC and Dagshub
        run: |
          pip install --quiet dvc[s3] dagshub

      - name: Pull DVC data from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc pull --quiet
          test -f artifacts/data/vectorized/test_vectorized.csv         #test if file got downloaded from s3

      - name: Run DVC pipeline
        env:
          PYTHONPATH: ${{ github.workspace }}                           #pythonpath tells to locate src folder
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          dvc repro --quiet

      - name: Run model loading tests
        env:
          DAGSHUB_PAT: ${{ secrets.DAGSHUB_PAT }}
        run: |
          python -m unittest discover -s tests -p "test_model.py"
